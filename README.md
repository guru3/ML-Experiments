# ML-Experiments

Trying different things out usually inspired from articles on machine learning topics


##### Unless the machine learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem!

##### Generating sequential data is the closest computers get to dreaming

##### Diversity of models is what make ensembling work

##### Hyperparameter optimization is technically a way to overfit validation data

##### VAEs  are  great  for  learning  latent spaces that are well structured, where specific directions encode a meaningful axis of variation in the data. GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity.

##### The only real success of deep learning so far has been the ability to map space X to space Y using a continuous geometric transform, given large amounts of human-annotated data. Doing this well is a game-changer for essentially every industry, but itâ€™s still a long way from human-level AI.
